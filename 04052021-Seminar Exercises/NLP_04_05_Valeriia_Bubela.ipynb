{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex 1. nltk text generation\n",
    "Train an ngram model on the english bible and get it to generate random text. Experiment with different start words and window lengths.\n",
    "How intelligible is the text?\n",
    "Discuss the strengths and weaknesses of this method of generating random text and propose alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/valeriia/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never\n",
      "be\n",
      "a\n",
      "man\n",
      ",\n",
      "and\n",
      "the\n",
      "LORD\n",
      ",\n",
      "and\n",
      "the\n",
      "LORD\n",
      ",\n",
      "and\n",
      "the\n",
      "LORD\n",
      ",\n",
      "and\n",
      "the\n",
      "LORD\n"
     ]
    }
   ],
   "source": [
    "bible = nltk.corpus.gutenberg.words('bible-kjv.txt')\n",
    "def generate_model(cfdist, word, num=20):\n",
    "    for i in range(num):\n",
    "        print(word),\n",
    "        word = cfdist[word].max()\n",
    "        \n",
    "bigrams = nltk.bigrams(bible)\n",
    "cfd = nltk.ConditionalFreqDist(bigrams)\n",
    "\n",
    "generate_model(cfd, \"never\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex 2. wn meronyms\n",
    "Use the BROWN religion vs. romance subcorpora.\n",
    "Which one has more NOUN meronyms? Remember that there are two types of meronyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Religion vocab: 5875 , meronyms: 358 , meronyms %: 0.060936170212765955\n",
      "Romance vocab: 7916 , meronyms: 804 , meronyms %: 0.10156644770085901\n"
     ]
    }
   ],
   "source": [
    "words_rel = brown.words(categories=\"religion\")\n",
    "words_rom = brown.words(categories=\"romance\")\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def meronyms_count(text):\n",
    "    text_lemmatized = [lemmatizer.lemmatize(w) for w in text]\n",
    "    counter = 0\n",
    "    for w in set(text_lemmatized):\n",
    "        for synset in wn.synsets(w, wn.NOUN):\n",
    "            for meronym in wn.synset(synset.name()).part_meronyms() or wn.synset(synset.name()).substance_meronyms():\n",
    "                if meronym.name()[:-5] in text_lemmatized:\n",
    "                    counter += 1\n",
    "    return len(set(text_lemmatized)), counter\n",
    "\n",
    "print(\"Religion vocab:\",meronyms_count(words_rel)[0],\",\", \"meronyms:\",meronyms_count(words_rel)[1],\",\",\"meronyms %:\",meronyms_count(words_rel)[1]/meronyms_count(words_rel)[0])\n",
    "print(\"Romance vocab:\",meronyms_count(words_rom)[0],\",\", \"meronyms:\",meronyms_count(words_rom)[1],\",\",\"meronyms %:\",meronyms_count(words_rom)[1]/meronyms_count(words_rom)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex 3. wn polysemy\n",
    "a) The polysemy of a word is the number of senses it has. Using the BROWN religion vs. romance subcorpora, which subcorpus is generally more polysemous or ‘ambiguous’?\n",
    "b) Compute the average polysemy for the religion NOUNS in a 2 liner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Religion: avg. number of meanings per word: 5.287489361702128\n",
      "Romance: avg. number of meanings per word: 5.0828701364325415\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def ambiguity(text):\n",
    "    text_lemmatized = [lemmatizer.lemmatize(w) for w in text]\n",
    "    counter = 0\n",
    "    for w in set(text_lemmatized):\n",
    "        for synset in wn.synsets(w):\n",
    "            counter += 1\n",
    "    return counter/len(set(text_lemmatized))\n",
    "\n",
    "from nltk.corpus import brown\n",
    "words_rel = brown.words(categories=\"religion\")\n",
    "words_rom = brown.words(categories=\"romance\")\n",
    "print(\"Religion: avg. number of meanings per word:\",ambiguity(words_rel))\n",
    "print(\"Romance: avg. number of meanings per word:\",ambiguity(words_rom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex 4. wn word similarity\n",
    "wn provides 6 different measures of word similarity.\n",
    "Which of these measures predicts the human ratings from the ‘simlex999’ gold standard best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>POS</th>\n",
       "      <th>SimLex999</th>\n",
       "      <th>conc(w1)</th>\n",
       "      <th>conc(w2)</th>\n",
       "      <th>concQ</th>\n",
       "      <th>Assoc(USF)</th>\n",
       "      <th>SimAssoc333</th>\n",
       "      <th>SD(SimLex)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>old</td>\n",
       "      <td>new</td>\n",
       "      <td>A</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smart</td>\n",
       "      <td>intelligent</td>\n",
       "      <td>A</td>\n",
       "      <td>9.20</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard</td>\n",
       "      <td>difficult</td>\n",
       "      <td>A</td>\n",
       "      <td>8.77</td>\n",
       "      <td>3.76</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2</td>\n",
       "      <td>5.94</td>\n",
       "      <td>1</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>cheerful</td>\n",
       "      <td>A</td>\n",
       "      <td>9.55</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1</td>\n",
       "      <td>5.85</td>\n",
       "      <td>1</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hard</td>\n",
       "      <td>easy</td>\n",
       "      <td>A</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3.76</td>\n",
       "      <td>2.07</td>\n",
       "      <td>2</td>\n",
       "      <td>5.82</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>join</td>\n",
       "      <td>acquire</td>\n",
       "      <td>V</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.86</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>send</td>\n",
       "      <td>attend</td>\n",
       "      <td>V</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.70</td>\n",
       "      <td>3.17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>gather</td>\n",
       "      <td>attend</td>\n",
       "      <td>V</td>\n",
       "      <td>4.80</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>absorb</td>\n",
       "      <td>withdraw</td>\n",
       "      <td>V</td>\n",
       "      <td>2.97</td>\n",
       "      <td>3.11</td>\n",
       "      <td>3.04</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>attend</td>\n",
       "      <td>arrive</td>\n",
       "      <td>V</td>\n",
       "      <td>6.08</td>\n",
       "      <td>3.17</td>\n",
       "      <td>3.22</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word1        word2 POS  SimLex999  conc(w1)  conc(w2)  concQ  \\\n",
       "0       old          new   A       1.58      2.72      2.81      2   \n",
       "1     smart  intelligent   A       9.20      1.75      2.46      1   \n",
       "2      hard    difficult   A       8.77      3.76      2.21      2   \n",
       "3     happy     cheerful   A       9.55      2.56      2.34      1   \n",
       "4      hard         easy   A       0.95      3.76      2.07      2   \n",
       "..      ...          ...  ..        ...       ...       ...    ...   \n",
       "994    join      acquire   V       2.85      2.86      2.93      2   \n",
       "995    send       attend   V       1.67      2.70      3.17      2   \n",
       "996  gather       attend   V       4.80      2.75      3.17      2   \n",
       "997  absorb     withdraw   V       2.97      3.11      3.04      2   \n",
       "998  attend       arrive   V       6.08      3.17      3.22      2   \n",
       "\n",
       "     Assoc(USF)  SimAssoc333  SD(SimLex)  \n",
       "0          7.25            1        0.41  \n",
       "1          7.11            1        0.67  \n",
       "2          5.94            1        1.19  \n",
       "3          5.85            1        2.18  \n",
       "4          5.82            1        0.93  \n",
       "..          ...          ...         ...  \n",
       "994        0.00            0        0.99  \n",
       "995        0.00            0        1.44  \n",
       "996        0.00            0        1.97  \n",
       "997        0.00            0        1.75  \n",
       "998        0.00            0        1.18  \n",
       "\n",
       "[999 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "simlex999 = pd.read_csv(\"SimLex-999.txt\", sep=\"\\t\")\n",
    "simlex999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path (0.4388878679612576, 4.15584005221031e-43)\n",
      "lch (0.4339421864617497, 4.461004522769442e-42)\n",
      "wup (0.3505600823618936, 4.4910144954556744e-27)\n",
      "res (0.11777705210549169, 0.00043648545526834523)\n",
      "jcn (0.23758159166048723, 7.350903788703595e-13)\n",
      "lin (0.2831489470556223, 7.805533206291413e-18)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from nltk.corpus import genesis\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "simlex_scores = []\n",
    "wordnet_scores = []\n",
    "for x, y, pos, simlex_score in zip(simlex999.word1, simlex999.word2, simlex999.POS, simlex999.SimLex999):\n",
    "    w1 = wn.synset(\"{}.{}.01\".format(x, pos.lower()))\n",
    "    w2 = wn.synset(\"{}.{}.01\".format(y, pos.lower()))\n",
    "    try:\n",
    "        path = w1.path_similarity(w2)\n",
    "        lch = w1.lch_similarity(w2)\n",
    "        wup = w1.wup_similarity(w2)\n",
    "        res = w1.res_similarity(w2, genesis_ic)\n",
    "        jcn = w1.jcn_similarity(w2, genesis_ic)\n",
    "        lin = w1.lin_similarity(w2, genesis_ic)\n",
    "        wn_scores = [path, lch, wup, res, jcn, lin]\n",
    "        for s in wn_scores:\n",
    "            if s == None:\n",
    "                raise ValueError(\"None\")\n",
    "        simlex_scores.append(simlex_score)\n",
    "        wordnet_scores.append(wn_scores)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "simlex_scores = np.asarray(simlex_scores)  \n",
    "wordnet_scores = np.asarray(wordnet_scores)\n",
    "n = wordnet_scores.shape[1]\n",
    "wordnet_measures = [\"path\", \"lch\", \"wup\", \"res\", \"jcn\", \"lin\"]\n",
    "for i in range(n):\n",
    "    print(wordnet_measures[i],stats.pearsonr(wordnet_scores[:,i],simlex_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
